# æ„å›¾è¯†åˆ«è®­ç»ƒé¡¹ç›®

åŸºäº LLaMA-Factory çš„æ„å›¾è¯†åˆ«æ¨¡å‹è®­ç»ƒé¡¹ç›®ï¼Œä¸“é—¨ç”¨äºè®­ç»ƒé«˜æ•ˆã€è½»é‡çš„æ„å›¾è¯†åˆ«æ¨¡å‹ã€‚

## ğŸ¯ é¡¹ç›®ç›®æ ‡

- **ä¸»è¦ç›®æ ‡**ï¼šè®­ç»ƒä¸“é—¨ç”¨äºæ„å›¾è¯†åˆ«çš„è½»é‡çº§æ¨¡å‹
- **åº”ç”¨åœºæ™¯**ï¼šæ™ºèƒ½å®¢æœã€è¯­éŸ³åŠ©æ‰‹ã€ä»»åŠ¡è‡ªåŠ¨åŒ–
- **æ€§èƒ½æŒ‡æ ‡**ï¼šå‡†ç¡®ç‡ > 90%ï¼Œå“åº”æ—¶é—´ < 100ms
- **éƒ¨ç½²è¦æ±‚**ï¼šæ”¯æŒ CPU/GPU æ¨ç†ï¼Œå†…å­˜å ç”¨ < 2GB

## ğŸ¤– æ¨¡å‹é€‰æ‹©

### æ¨èæ¨¡å‹ï¼šDeepSeek-R1-Distill-Qwen-1.5B
- **è·¯å¾„**ï¼š`/workspace/models/DeepSeek-R1-Distill-Qwen-1.5B/`
- **å¤§å°**ï¼š3.31 GB
- **ä¼˜åŠ¿**ï¼šè½»é‡çº§ã€ä¸­æ–‡ç†è§£èƒ½åŠ›å¼ºã€å·²é¢„è£…

### å¤‡é€‰æ¨¡å‹ï¼šQwen2.5-7B-Instruct
- **è·¯å¾„**ï¼š`/workspace/models/Qwen2.5-7B-Instruct/`
- **å¤§å°**ï¼š14.18 GB
- **é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦æ›´é«˜ç²¾åº¦çš„ç”Ÿäº§ç¯å¢ƒ

## ğŸ“Š æ•°æ®é›†

### 1. ATIS æ•°æ®é›†ï¼ˆèˆªç©ºé¢†åŸŸï¼‰
- **æ ·æœ¬æ•°**ï¼š5,000 è®­ç»ƒ + 1,000 æµ‹è¯•
- **æ„å›¾ç±»å‹**ï¼š26 ç§
- **ç‰¹ç‚¹**ï¼šæ ‡å‡†åŸºå‡†ï¼Œä¾¿äºå¯¹æ¯”

### 2. SNIPS æ•°æ®é›†ï¼ˆæ™ºèƒ½åŠ©æ‰‹ï¼‰
- **æ ·æœ¬æ•°**ï¼š13,084 è®­ç»ƒ
- **æ„å›¾ç±»å‹**ï¼š7 ç§æ ¸å¿ƒæ„å›¾
- **ç‰¹ç‚¹**ï¼šè´´è¿‘å®é™…åº”ç”¨

### 3. CrossWOZ æ•°æ®é›†ï¼ˆä¸­æ–‡å¤šé¢†åŸŸï¼‰
- **æ ·æœ¬æ•°**ï¼š5K å¯¹è¯ï¼Œ30K+ è¯­å¥
- **é¢†åŸŸ**ï¼šé¤é¥®ã€ç”µå½±ã€é…’åº—ç­‰ 5 ä¸ªé¢†åŸŸ
- **ç‰¹ç‚¹**ï¼šä¸­æ–‡åœºæ™¯è¦†ç›–

## ğŸ› ï¸ è®­ç»ƒæµç¨‹

### é˜¶æ®µä¸€ï¼šåŸºç¡€æŒ‡ä»¤å¾®è°ƒ
```bash
# ä½¿ç”¨ LoRA å¾®è°ƒ
llamafactory-cli train config/model_config.yaml
```

### é˜¶æ®µäºŒï¼šé¢†åŸŸè‡ªé€‚åº”
```bash
# ä½¿ç”¨ QLoRA å¾®è°ƒï¼Œé™ä½æ˜¾å­˜éœ€æ±‚
llamafactory-cli train config/training_config.yaml
```

### é˜¶æ®µä¸‰ï¼šæ¨ç†æµ‹è¯•
```bash
# æµ‹è¯•æ¨¡å‹æ•ˆæœ
llamafactory-cli chat model_name_or_path=outputs/models/intent_recognition_lora
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
intent_recognition/
â”œâ”€â”€ README.md                    # é¡¹ç›®è¯´æ˜
â”œâ”€â”€ config/                      # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ model_config.yaml        # æ¨¡å‹é…ç½®
â”‚   â”œâ”€â”€ dataset_config.yaml      # æ•°æ®é›†é…ç½®
â”‚   â””â”€â”€ training_config.yaml     # è®­ç»ƒé…ç½®
â”œâ”€â”€ data/                        # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ raw/                     # åŸå§‹æ•°æ®
â”‚   â”œâ”€â”€ processed/               # å¤„ç†åæ•°æ®
â”‚   â””â”€â”€ dataset_info.json        # LLaMA-Factory æ•°æ®é›†é…ç½®
â”œâ”€â”€ scripts/                     # è„šæœ¬ç›®å½•
â”‚   â”œâ”€â”€ download_dataset.py      # æ•°æ®ä¸‹è½½
â”‚   â”œâ”€â”€ preprocess_data.py       # æ•°æ®é¢„å¤„ç†
â”‚   â””â”€â”€ evaluate.py              # æ¨¡å‹è¯„ä¼°
â”œâ”€â”€ outputs/                     # è¾“å‡ºç›®å½•
â”‚   â”œâ”€â”€ models/                  # è®­ç»ƒå®Œæˆçš„æ¨¡å‹
â”‚   â”œâ”€â”€ logs/                    # è®­ç»ƒæ—¥å¿—
â”‚   â””â”€â”€ evaluations/             # è¯„ä¼°ç»“æœ
â””â”€â”€ examples/                    # ç¤ºä¾‹ç›®å½•
    â”œâ”€â”€ inference_example.py     # æ¨ç†ç¤ºä¾‹
    â””â”€â”€ api_example.py          # API ä½¿ç”¨ç¤ºä¾‹
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡
```bash
cd /workspace/intent_recognition
source /workspace/hf_venv/bin/activate
```

### 2. æ•°æ®å‡†å¤‡
```bash
python scripts/download_dataset.py
python scripts/preprocess_data.py
```

### 3. å¼€å§‹è®­ç»ƒ
```bash
cd /workspace/LLaMA-Factory
llamafactory-cli train /workspace/intent_recognition/config/model_config.yaml
```

### 4. æ¨¡å‹è¯„ä¼°
```bash
python /workspace/intent_recognition/scripts/evaluate.py
```

## ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡

- **å‡†ç¡®ç‡** (Accuracy) > 90%
- **ç²¾ç¡®ç‡** (Precision) > 85%
- **å¬å›ç‡** (Recall) > 85%
- **F1 åˆ†æ•°** (F1-Score) > 85%

## ğŸ”§ æŠ€æœ¯æ ˆ

- **æ¡†æ¶**ï¼šLLaMA-Factory
- **æ¨¡å‹**ï¼šDeepSeek-R1-Distill-Qwen-1.5B
- **å¾®è°ƒæ–¹æ³•**ï¼šLoRAã€QLoRA
- **æ•°æ®å¤„ç†**ï¼špandasã€jiebaã€NLTK
- **è¯„ä¼°å·¥å…·**ï¼šsklearnã€transformers

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### Python æ¨ç†ç¤ºä¾‹
```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("outputs/models/intent_recognition_lora")
model = AutoModelForCausalLM.from_pretrained("outputs/models/intent_recognition_lora")

def recognize_intent(text):
    prompt = f"è¯·è¯†åˆ«ä»¥ä¸‹ç”¨æˆ·æ„å›¾ï¼š{text}\næ„å›¾ï¼š"
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=50)
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return result.split("æ„å›¾ï¼š")[-1].strip()

# ä½¿ç”¨ç¤ºä¾‹
intent = recognize_intent("å¸®æˆ‘æŸ¥è¯¢æ˜å¤©åŒ—äº¬åˆ°ä¸Šæµ·çš„èˆªç­")
print(f"è¯†åˆ«æ„å›¾ï¼š{intent}")
```

### API è°ƒç”¨ç¤ºä¾‹
```bash
curl -X POST "http://localhost:8000/api/v1/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "intent_recognition_lora",
    "messages": [
      {"role": "user", "content": "å¸®æˆ‘æŸ¥è¯¢æ˜å¤©åŒ—äº¬åˆ°ä¸Šæµ·çš„èˆªç­"}
    ]
  }'
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. Fork æœ¬é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/new-intent`)
3. æäº¤æ›´æ”¹ (`git commit -am 'Add new intent type'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/new-intent`)
5. åˆ›å»º Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäº MIT è®¸å¯è¯å¼€æºã€‚
# LLaMA-Factory éƒ¨ç½²é¡¹ç›®

è¿™æ˜¯ä¸€ä¸ªåŸºäº [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) çš„å¤§æ¨¡å‹å¾®è°ƒæ¡†æ¶éƒ¨ç½²é¡¹ç›®ï¼Œæä¾›äº†é›¶ä»£ç å¾®è°ƒç™¾ä½™ç§å¤§æ¨¡å‹çš„å®Œæ•´è§£å†³æ–¹æ¡ˆã€‚

## ğŸš€ é¡¹ç›®ç‰¹æ€§

- **é›¶ä»£ç å¾®è°ƒ**ï¼šæä¾› Web UI å’Œå‘½ä»¤è¡Œä¸¤ç§æ“ä½œæ–¹å¼
- **æ”¯æŒç™¾ä½™ç§å¤§æ¨¡å‹**ï¼šåŒ…æ‹¬ LLaMAã€Qwenã€DeepSeek ç­‰
- **å¤šç§å¾®è°ƒæ–¹æ³•**ï¼šæ”¯æŒ LoRAã€QLoRAã€å…¨å‚æ•°å¾®è°ƒç­‰
- **å¤šæ¨¡æ€æ”¯æŒ**ï¼šæ”¯æŒæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ç­‰å¤šæ¨¡æ€æ•°æ®
- **åˆ†å¸ƒå¼è®­ç»ƒ**ï¼šæ”¯æŒå¤š GPUã€å¤šèŠ‚ç‚¹è®­ç»ƒ
- **é‡åŒ–è®­ç»ƒ**ï¼šæ”¯æŒ INT8ã€INT4ã€GPTQã€AWQ ç­‰é‡åŒ–æ–¹æ¡ˆ

## ğŸ“‹ ç¯å¢ƒè¦æ±‚

- **Python**: 3.11.1
- **CUDA**: 12.1 (æ¨è GPU ç¯å¢ƒ)
- **å†…å­˜**: å»ºè®® 16GB+ RAM
- **å­˜å‚¨**: å»ºè®® 50GB+ å¯ç”¨ç©ºé—´

## ğŸ› ï¸ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå®‰è£…

```bash
# å…‹éš†é¡¹ç›®
git clone <your-repo-url>
cd <project-name>

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ–
venv\Scripts\activate     # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. å¯åŠ¨æ–¹å¼

#### Web UI æ¨¡å¼ï¼ˆæ¨èï¼‰

```bash
cd LLaMA-Factory
python src/webui.py
```

è®¿é—®åœ°å€ï¼š`http://127.0.0.1:7860`

#### API æœåŠ¡æ¨¡å¼

```bash
cd LLaMA-Factory
python src/api.py
```

API æ–‡æ¡£ï¼š`http://localhost:8000/docs`

#### å‘½ä»¤è¡Œè®­ç»ƒæ¨¡å¼

```bash
# ä½¿ç”¨é…ç½®æ–‡ä»¶è®­ç»ƒ
llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml

# æˆ–ç›´æ¥ä½¿ç”¨å‚æ•°è®­ç»ƒ
llamafactory-cli train \
  model_name_or_path=models/Qwen2.5-7B-Instruct \
  dataset=identity,alpaca_en_demo \
  finetuning_type=lora \
  output_dir=saves/qwen2.5-7b/lora/sft
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
â”œâ”€â”€ LLaMA-Factory/           # ä¸»é¡¹ç›®ç›®å½•
â”‚   â”œâ”€â”€ src/                 # æºä»£ç 
â”‚   â”‚   â”œâ”€â”€ api.py          # API æœåŠ¡å…¥å£
â”‚   â”‚   â”œâ”€â”€ train.py        # è®­ç»ƒå…¥å£
â”‚   â”‚   â”œâ”€â”€ webui.py        # Web UI å…¥å£
â”‚   â”‚   â””â”€â”€ llamafactory/   # æ ¸å¿ƒæ¨¡å—
â”‚   â”œâ”€â”€ data/               # æ•°æ®é›†å’Œé…ç½®
â”‚   â”œâ”€â”€ examples/           # ç¤ºä¾‹é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ scripts/            # å·¥å…·è„šæœ¬
â”‚   â””â”€â”€ requirements.txt    # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ models/                 # é¢„è®­ç»ƒæ¨¡å‹
â”‚   â”œâ”€â”€ DeepSeek-R1-Distill-Qwen-1.5B/
â”‚   â””â”€â”€ Qwen2.5-7B-Instruct/
â”œâ”€â”€ frp/                    # å†…ç½‘ç©¿é€å·¥å…·
â””â”€â”€ README.md              # é¡¹ç›®è¯´æ˜
```

## ğŸ¯ é¢„è£…æ¨¡å‹

é¡¹ç›®å·²é¢„è£…ä»¥ä¸‹æ¨¡å‹ï¼š

1. **DeepSeek-R1-Distill-Qwen-1.5B** (3.31 GB)
   - è·¯å¾„ï¼š`models/DeepSeek-R1-Distill-Qwen-1.5B/`
   - é€‚åˆå¿«é€Ÿæµ‹è¯•å’ŒåŸå‹å¼€å‘

2. **Qwen2.5-7B-Instruct** (14.18 GB)
   - è·¯å¾„ï¼š`models/Qwen2.5-7B-Instruct/`
   - é€‚åˆç”Ÿäº§ç¯å¢ƒä½¿ç”¨

## ğŸ”§ é…ç½®è¯´æ˜

### ç¯å¢ƒé…ç½®

ä¸»è¦é…ç½®æ–‡ä»¶ï¼š`LLaMA-Factory/.env.local`

```env
# API æœåŠ¡é…ç½®
API_HOST=0.0.0.0
API_PORT=8000

# Web UI é…ç½®
GRADIO_SERVER_PORT=7860
GRADIO_SHARE=false

# åˆ†å¸ƒå¼è®­ç»ƒé…ç½®
MASTER_ADDR=localhost
MASTER_PORT=29500
```

### æ•°æ®é›†é…ç½®

æ•°æ®é›†é…ç½®æ–‡ä»¶ï¼š`LLaMA-Factory/data/dataset_info.json`

```json
{
  "train": {
    "file_name": "train_change.json",
    "formatting": "sharegpt"
  },
  "eval": {
    "file_name": "eval_change.json", 
    "formatting": "sharegpt"
  }
}
```

## ğŸš€ å†…ç½‘ç©¿é€

å¦‚æœéœ€è¦ä»å¤–ç½‘è®¿é—® Web UIï¼Œå¯ä»¥ä½¿ç”¨é¡¹ç›®å†…ç½®çš„ FRP å·¥å…·ï¼š

### æœåŠ¡å™¨ç«¯é…ç½®

```bash
cd frp/frp_0.65.0_linux_amd64/
./frps -c frps.toml
```

### å®¢æˆ·ç«¯é…ç½®

```bash
cd frp/frp_0.65.0_linux_amd64/
./frpc -c frpc.toml
```

## ğŸ¨ åŠŸèƒ½ç‰¹æ€§

### æ”¯æŒçš„å¾®è°ƒæ–¹æ³•

- **LoRA**: ä½ç§©é€‚åº”ï¼Œå‚æ•°é«˜æ•ˆå¾®è°ƒ
- **QLoRA**: é‡åŒ– LoRAï¼Œå†…å­˜å‹å¥½
- **å…¨å‚æ•°å¾®è°ƒ**: å®Œæ•´æ¨¡å‹è®­ç»ƒ
- **DPO**: ç›´æ¥åå¥½ä¼˜åŒ–
- **PPO**: è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–
- **KTO**: å¡å°¼æ›¼-ç‰¹æ²ƒæ–¯åŸºä¼˜åŒ–

### æ”¯æŒçš„æ¨¡å‹æ¶æ„

- LLaMA ç³»åˆ— (LLaMA-2, LLaMA-3, LLaMA-4)
- Qwen ç³»åˆ—
- DeepSeek ç³»åˆ—
- Mixtral
- Baichuan
- ChatGLM
- å…¶ä»– 100+ æ¨¡å‹

### é«˜çº§åŠŸèƒ½

- **å¤šæ¨¡æ€æ”¯æŒ**: å›¾æ–‡ã€éŸ³è§†é¢‘å¤„ç†
- **åˆ†å¸ƒå¼è®­ç»ƒ**: æ”¯æŒå¤š GPUã€å¤šèŠ‚ç‚¹
- **é‡åŒ–è®­ç»ƒ**: INT8ã€INT4ã€GPTQã€AWQ
- **å†…å­˜ä¼˜åŒ–**: Flash Attentionã€Gradient Checkpointing
- **æ¨ç†åŠ é€Ÿ**: vLLMã€SGLang åç«¯

## ğŸ“š ä½¿ç”¨ç¤ºä¾‹

### 1. LoRA å¾®è°ƒç¤ºä¾‹

```bash
llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
```

### 2. ä½¿ç”¨æœ¬åœ°æ¨¡å‹

```bash
llamafactory-cli train \
  model_name_or_path=models/Qwen2.5-7B-Instruct \
  dataset=identity,alpaca_en_demo \
  finetuning_type=lora \
  output_dir=saves/qwen2.5-7b/lora/sft
```

### 3. æ¨¡å‹æ¨ç†

```bash
llamafactory-cli chat \
  model_name_or_path=saves/qwen2.5-7b/lora/sft \
  template=qwen
```

## ğŸ› ï¸ å¼€å‘å·¥å…·

é¡¹ç›®æä¾›äº†å®Œæ•´çš„å¼€å‘å·¥å…·é“¾ï¼š

```bash
# ä»£ç æ ¼å¼åŒ–å’Œæ£€æŸ¥
ruff format .
ruff check .

# è¿è¡Œæµ‹è¯•
pytest

# æ„å»ºåŒ…
python -m build

# é¢„æäº¤é’©å­
pre-commit install
```

## ğŸ“– ç›¸å…³æ–‡æ¡£

- [LLaMA-Factory å®˜æ–¹æ–‡æ¡£](https://llamafactory.readthedocs.io/)
- [API æ¥å£æ–‡æ¡£](http://localhost:8000/docs)
- [é…ç½®æ–‡ä»¶ç¤ºä¾‹](LLaMA-Factory/examples/)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäº MIT è®¸å¯è¯å¼€æºã€‚

## ğŸ”— ç›¸å…³é“¾æ¥

- [LLaMA-Factory GitHub](https://github.com/hiyouga/LLaMA-Factory)
- [Hugging Face](https://huggingface.co/)
- [PyTorch](https://pytorch.org/)

---

**æ³¨æ„**: é¦–æ¬¡è¿è¡Œå‰è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…æ‰€æœ‰ä¾èµ–ï¼Œå¹¶æ ¹æ®ç¡¬ä»¶é…ç½®è°ƒæ•´ç›¸å…³å‚æ•°ã€‚